---
title: "Fitting models to unknown DGPs"
author: "Alex Towell"
date: "2022-10-14"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Fitting models to unknown DGPs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{ggplot2}
  %\VignetteDepends{tibble}
---

```{r, include=F}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction
We are interested in the generative process that gave rise to the data we observed.
In the real world, systems are quite complex, but we settle for simpler models
for analytical tractability. So, we usually assume:

1. The sample is i.i.d.

2. The observations may be reasonably modeled by some relatively simple process.

3. Other factors, like ambient temperature, are either negligible or are more or
   less constant in the sample.

Since this is a simulation, we know the underlying DGP (data generating process).
It's just
$$
    T_i = W_i + \epsilon_i
$$
where
$$
    W_i \sim \operatorname{weibull}(k,\lambda)
$$
and
$$
    \epsilon_i \sim \operatorname{normal}(0,\sigma).
$$

In the real world, we do not know the DGP. In this study, we will assume that
either $T_1,\ldots,T_n$ comes from Weibull or Normal. Clearly, the true
DGF is a bit more complicated but still very simple compared to a more realistic
DGP.

Then, the process of parametrically modeling the observed data may take the
following steps:

1. Visualize the data, e.g., plot a histogram of the data.

2. Guess which parametric distribution (for the components) might fit the
   observed data for the system lifetime.
   
3. Use a statistical test for goodness-of-fit.

4. Repeat steps 2 and 3 if the measure of goodness of fit is not satisfactory.

## Simulation parameters and generation
The simulation parameters are given by:
```{r setup}
library(tibble)
library(stats)

sim.n <- 27
sim.err.sd <- 0.1
sim.shape <- 20
sim.scale <- 3
sim.theta = c(sim.shape,sim.scale)
set.seed(142334)
```

We generate the data with the following R code:
```{r}
sim.df <- tibble(lifetime=
  rweibull(n=sim.n, shape=sim.shape, scale=sim.scale) +
  rnorm(n=sim.n, mean=0, sd=sim.err.sd))
```

A few elements from the sample are given by:
```{r, echo=F}
head(sim.df)
```

## Visualizing the data
Visualizing the data is a good first step in the analysis of the data.
If the data is univariate or bivariate, we can plot a histogram of the data
pretty easily (if it's multivariate, we can plot the marginal distributions
of the data).

We show a histogram of the simulated data below:
```{r histo, fig.align='center', echo=F}
library(ggplot2)
ggplot(sim.df,aes(x=lifetime)) +
    geom_histogram(color="dark blue",
                   fill="light blue",
                   bins=25) +
    labs(title="Simulated Lifetime Data",
         subtitle="Histogram")
```

## Parametrically modeling the data
If we only had this sample, what might we conclude?
This can be a very difficult problem.
In our case, we know that the simulated data is drawn from the distribution
$T_i = W_i + \epsilon_i$ where
$$
  W_i \sim \operatorname{weibull}(\lambda = `r sim.shape`,
                                k = `r sim.scale`)
$$
and
$$
  \epsilon_i \sim \operatorname{normal}(\mu=0,\sigma=`r sim.err.sd`).
$$
However, in real-world data sets, we do not know the distribution. So, let us
suppose that we do not know the true distribution of the data.

If we were only interested in, say, *prediction*, and we had a sufficiently
large sample, we could use a non-parametric methods and "let the data speak for
itself." However, if we are interested in inference (e.g., explaining the
data) or the sample was small, then we usually need to make some assumptions
about the data.

In this case, we will assume that the data is drawn from a parametric
distribution. There are many well-known, named parametric distributions, e.g.,
Pareto, Weibull, and Normal, to name a few.
From experience, it seems like the Weibull and the normal might be good fits
to the data. However, note that since the normal distribution permits negative
values to be realized, it may not be an appropriate choice. Still, since these
are only approximations anyway, this may not be a big deal.

## Maximum likelihood estimation
First, let us fit the Weibull distribution by choosing appropriate shape
$\lambda$ and scale $k$ parameters using the maximum likelihood estimator.

To find the MLE of $\theta = (\lambda,k)'$, we need the log-likelihood function,
which is given by the following R code:
```{r eval=F}
loglike <- function(theta) sum(dweibull(
  sim.df$lifetime, shape=theta[1], scale=theta[2], log=TRUE))
```

This is the definition of the log-likelihood, where we have used the built-in
R function `dweibull` to compute the sum of the log-density function for each
observation. However, we can also use the `algebraic.mle` package to compute
the log-likelihood function in a more efficient manner.
```{r}
library(algebraic.mle)
ll.wei <- weibull_loglike(sim.df$lifetime)
```

An MLE is a point $(\hat k,\hat\lambda)'$ that is a maximum of the log-likelihood
function over the support of the parameters. In cases where a closed solution isn't
possible, we use iterative techniques.

A popular choice is Newton-Raphson, which is a local search method that relies upon
both the gradient and the Hessian of the log-likelihood (Jocobian of the gradient) to
iteratively improve the solution to the MLE equation. Here is an example of
Newton-raphson code:
```{r eval=F}
# f is the function we want to find the root of
# Jf is the jocabian of f
# x0 is the starting point
newton_raphson <- function(f, Jf, x0, iter=100)
{
  for (i in 1:iter)
  {
    fx <- f(x0)              # new function value
    d <- solve(Jf(x0), fx)   # newton-raphson direction (pointing uphill)
    x0 <- x0 - d             # newton-raphson update
  }
  x0
}
```

Newton-Raphson and other local search methods need a good starting point 
(that is already close to the MLE) to converge to the MLE. To find a good
starting point, we use a global search method, Simulated Annealing, implemented
by `sim_anneal` function. Here is the code for finding a good starting point:
```{r}
# find a good starting position
start <- sim_anneal(
    fn = ll.wei,
    par = sim.theta,
    control = list(
        t_init=100,
        t_end=.1,
        fnscale=-1,
        it_per_temp=20,
        maxit=10000L,
        sup=function(theta) all(theta > 0),
        trace=TRUE))
cat("initial guess (k0,lambda0)' = (", start$par, ")'.\n")
```

To get an idea about what the algorithm is doing, we can plot the trace of the
algorithm. Let's take a look at some plots.
```{r, fig.align='center', fig.height=4, fig.width=6, echo=F}
library(ggplot2)
m <- 2
# Convert the matrix to a data frame
data_df <- as_tibble(start$trace_info)
data_df$best <- as.factor(data_df$best)
# Reshape data for ggplot2
library(reshape2)
long_data <- melt(data_df,
    id.vars = c("it", "value", "temp", "best"),
    variable.name = "parameter",
    value.name = "value_par")

# Convergence plot (1)
convergence_plot <- ggplot(data_df, aes(x = it, y = value, color = best)) +
  geom_line() +
  labs(title = "Convergence Plot",
       x = "Iteration",
       y = "Best Function Value") +
  scale_color_discrete(name = "Best Value", labels = c("No", "Yes"))

print(convergence_plot)

# Parameter traces plot (3)
parameter_traces_plot <- ggplot(long_data, aes(x = it, y = value_par,
  color = best)) +
  geom_line() +
  facet_wrap(~parameter, ncol = m, scales = "free_y") +
  labs(title = "Parameter Traces",
       x = "Iteration",
       y = "Parameter Value") +
  scale_color_discrete(name = "Best Value", labels = c("No", "Yes"))

print(parameter_traces_plot)
```

In the convergence plot, we see the history of log-likelihood values as the
algorithm progresses. The left parameter trace plot shows the best
value for the shape parameter, $k$, as the algorithm progresses, and the right
plot does the same for the $\lambda$ parameter.

With this starting point in hand, we find an MLE with:
```{r}
library(algebraic.mle)
mle.wei <- mle_numerical(optim(
    par=start$par,
    fn=ll.wei,
    gr=weibull_score(sim.df$lifetime),
    method="BFGS",
    control=list(fnscale=-1)))
```

The `mle_numerical` function takes anything that is like an `optim` object and returns an `mle` object for it.

Here's a summary of it:
```{r}
summary(mle.wei)
```

