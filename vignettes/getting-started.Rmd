---
title: "Getting Started with numerical.mle"
author: "Alexander Towell"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Getting Started with numerical.mle}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.height = 4
)
```

## Introduction

The `numerical.mle` package provides numerical optimization algorithms for
maximum likelihood estimation (MLE). It offers a unified, composable API with:

- **Core solvers**: Gradient ascent and Newton-Raphson optimization
- **Meta-solvers**: Grid search and random restart for global optimization
- **Function transformers**: Subsampling for stochastic gradient ascent, penalty terms for regularization
- **Type-safe configuration**: Explicit configuration objects for convergence criteria

## Installation

```{r install, eval=FALSE}
# Install from GitHub
devtools::install_github("queelius/numerical.mle")
```

```{r load}
library(numerical.mle)
```

## Quick Start

### Basic Example: Normal Distribution MLE

Let's estimate the mean and standard deviation of normally distributed data:

```{r quickstart}
# Generate sample data
set.seed(123)
data <- rnorm(100, mean = 5, sd = 2)

# Define log-likelihood
loglike <- function(theta) {
  mu <- theta[1]
  sigma <- theta[2]
  if (sigma <= 0) return(-Inf)
  sum(dnorm(data, mean = mu, sd = sigma, log = TRUE))
}

# Define score function (gradient of log-likelihood)
score <- function(theta) {
  mu <- theta[1]
  sigma <- theta[2]
  n <- length(data)
  c(
    sum(data - mu) / sigma^2,
    -n / sigma + sum((data - mu)^2) / sigma^3
  )
}

# Constrain sigma to be positive
constraint <- mle_constraint(
  support = function(theta) theta[2] > 0,
  project = function(theta) c(theta[1], max(theta[2], 1e-6))
)

# Fit using gradient ascent with line search
result <- mle_gradient_ascent(
  loglike = loglike,
  score = score,
  theta0 = c(0, 1),
  config = mle_config_linesearch(max_iter = 100),
  constraint = constraint
)

# View results
cat("Estimated mean:", result$theta.hat[1], "(true: 5)\n")
cat("Estimated sd:", result$theta.hat[2], "(true: 2)\n")
cat("Sample mean:", mean(data), "\n")
cat("Sample sd:", sd(data), "\n")
```

## Configuration Objects

The package uses type-safe configuration objects to control optimization behavior.

### Base Configuration

```{r config-base}
# Basic configuration with convergence criteria
config <- mle_config(
  max_iter = 200,    # Maximum iterations
  rel_tol = 1e-6,    # Relative tolerance for convergence
  trace = FALSE,     # Store optimization path?
  debug = FALSE      # Print debug output?
)
```

### Gradient Configuration

For gradient-based methods, you can specify the learning rate:

```{r config-gradient}
config_grad <- mle_config_gradient(
  eta = 0.1,        # Learning rate / step size
  max_iter = 100,
  rel_tol = 1e-5
)
```
### Line Search Configuration

Line search adaptively finds good step sizes (recommended for most problems):

```{r config-linesearch}
config_ls <- mle_config_linesearch(
  max_step = 1.0,        # Maximum step size
  backtrack_ratio = 0.5, # Step reduction factor
  max_iter = 100,
  rel_tol = 1e-5
)
```

## Core Solvers

### Gradient Ascent

Gradient ascent uses the score function (gradient of log-likelihood) to
iteratively climb toward the maximum:

```{r gradient-ascent}
# Simple quadratic problem: maximize -(x^2 + y^2)
loglike <- function(theta) -(theta[1]^2 + theta[2]^2)
score <- function(theta) -2 * theta

result <- mle_gradient_ascent(
  loglike = loglike,
  score = score,
  theta0 = c(5, 5),
  config = mle_config_linesearch(max_iter = 50)
)

cat("Solution:", result$theta.hat, "\n")
cat("Iterations:", result$iter, "\n")
```

### Newton-Raphson

Newton-Raphson uses second-order information (Fisher information matrix) for
faster convergence:

```{r newton-raphson}
# Same problem with Newton-Raphson
fisher <- function(theta) matrix(c(2, 0, 0, 2), nrow = 2)

result_nr <- mle_newton_raphson(
  loglike = loglike,
  score = score,
  fisher = fisher,
  theta0 = c(5, 5),
  config = mle_config_linesearch(max_iter = 20)
)

cat("Solution:", result_nr$theta.hat, "\n")
cat("Iterations:", result_nr$iter, "\n")
```

### Convenience Wrappers

For quick prototyping, use the convenience wrappers:

```{r convenience}
# Quick gradient ascent
result <- mle_grad(loglike, score, theta0 = c(5, 5), max_iter = 50)

# Quick Newton-Raphson
result <- mle_nr(loglike, score, fisher, theta0 = c(5, 5), max_iter = 20)
```

## Constrained Optimization

Use `mle_constraint` to define domain constraints:

```{r constraints}
# Constrain parameters to be positive
constraint <- mle_constraint(
  support = function(theta) all(theta > 0),
  project = function(theta) pmax(theta, 1e-8)
)

# Box constraints [0, 10]
box_constraint <- mle_constraint(
  support = function(theta) all(theta >= 0 & theta <= 10),
  project = function(theta) pmax(0, pmin(10, theta))
)
```

## Meta-Solvers

### Grid Search

Grid search evaluates the likelihood on a grid, useful for finding good starting points:

```{r grid-search}
loglike <- function(theta) -(theta[1] - 3)^2 - (theta[2] + 2)^2

result <- mle_grid_search(
  loglike = loglike,
  lower = c(-5, -5),
  upper = c(5, 5),
  grid_size = 10
)

cat("Best grid point:", result$theta.hat, "\n")
cat("Points evaluated:", result$n_evaluated, "\n")
```

### Random Restart

Random restart runs multiple optimizations from random starting points,
helping escape local optima:

```{r random-restart}
loglike <- function(theta) -sum(theta^2)
score <- function(theta) -2 * theta

# Define how to sample starting points
sampler <- function() runif(2, -10, 10)

result <- mle_random_restart(
  loglike = loglike,
  solver = mle_grad,
  theta0_sampler = sampler,
  n_trials = 10,
  score = score,
  max_iter = 30
)

cat("Best solution:", result$theta.hat, "\n")
cat("Successful trials:", result$successful_trials, "/", result$n_trials, "\n")
```

## Function Transformers

### Stochastic Gradient Ascent

For large datasets, use subsampling to create stochastic gradients:

```{r subsampling, eval=FALSE}
# Large dataset
data <- rnorm(100000, mean = 5, sd = 2)

loglike_full <- function(theta, obs = data) {
  sum(dnorm(obs, mean = theta[1], sd = theta[2], log = TRUE))
}

# Subsample only 100 observations per iteration
loglike_stoch <- with_subsampling(
  loglike_full,
  data = data,
  subsample_size = 100
)

# Each evaluation uses a different random subsample
loglike_stoch(c(5, 2))
```

### Penalized Likelihood

Add regularization penalties to prevent overfitting:

```{r penalties}
loglike <- function(theta) -sum(theta^2)

# L1 penalty (LASSO)
loglike_l1 <- with_penalty(loglike, penalty_l1(), lambda = 0.1)

# L2 penalty (Ridge)
loglike_l2 <- with_penalty(loglike, penalty_l2(), lambda = 0.1)

# Elastic net (mix of L1 and L2)
loglike_enet <- with_penalty(
  loglike,
  penalty_elastic_net(alpha = 0.5),
  lambda = 0.1
)

# Compare values
theta <- c(1, 2, 3)
cat("Original:", loglike(theta), "\n")
cat("With L1:", loglike_l1(theta), "\n")
cat("With L2:", loglike_l2(theta), "\n")
cat("With Elastic Net:", loglike_enet(theta), "\n")
```

## Tracking Optimization Progress

Enable tracing to record the optimization path:

```{r trace}
loglike <- function(theta) -(theta[1]^2 + theta[2]^2)
score <- function(theta) -2 * theta

config <- mle_config_linesearch(max_iter = 20, trace = TRUE)

result <- mle_gradient_ascent(
  loglike = loglike,
  score = score,
  theta0 = c(5, 5),
  config = config
)

# The path is stored in result$path
if (!is.null(result$path)) {
  cat("Optimization path (first 5 steps):\n")
  print(head(result$path, 5))
}
```

## Complete Workflow Example

Here's a complete example fitting a Poisson regression:

```{r poisson, eval=FALSE}
# Simulate Poisson data
set.seed(42)
n <- 200
x <- runif(n, 0, 5)
true_beta <- c(0.5, 0.3)  # intercept and slope
lambda_true <- exp(true_beta[1] + true_beta[2] * x)
y <- rpois(n, lambda_true)

# Log-likelihood for Poisson regression
loglike <- function(beta) {
  eta <- beta[1] + beta[2] * x
  lambda <- exp(eta)
  sum(dpois(y, lambda, log = TRUE))
}

# Score function
score <- function(beta) {
  eta <- beta[1] + beta[2] * x
  lambda <- exp(eta)
  resid <- y - lambda
  c(sum(resid), sum(resid * x))
}

# Fit model
result <- mle_grad(
  loglike, score,
  theta0 = c(0, 0),
  max_iter = 100
)

cat("Estimated coefficients:", result$theta.hat, "\n")
cat("True coefficients:", true_beta, "\n")
```

## API Reference

The main functions in the package are:

**Configuration:**

- `mle_config()` - Base configuration
- `mle_config_gradient()` - Gradient descent configuration
- `mle_config_linesearch()` - Line search configuration
- `mle_constraint()` - Domain constraints

**Solvers:**

- `mle_gradient_ascent()` - Gradient ascent optimizer
- `mle_newton_raphson()` - Newton-Raphson optimizer
- `mle_grid_search()` - Grid search
- `mle_random_restart()` - Random restart wrapper

**Convenience:**

- `mle_grad()` - Quick gradient ascent
- `mle_nr()` - Quick Newton-Raphson

**Transformers:**

- `with_subsampling()` - Stochastic gradient via subsampling
- `with_penalty()` - Add penalty/regularization term
- `penalty_l1()`, `penalty_l2()`, `penalty_elastic_net()` - Penalty functions
