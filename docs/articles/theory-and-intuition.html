<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Theory and Intuition Behind Numerical MLE • compositional.mle</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Theory and Intuition Behind Numerical MLE">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">compositional.mle</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/getting-started.html">Getting Started</a></li>
    <li><a class="dropdown-item" href="../articles/theory-and-intuition.html">Theory and Intuition</a></li>
    <li><a class="dropdown-item" href="../articles/case-studies.html">Case Studies</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/queelius/compositional.mle/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Theory and Intuition Behind Numerical MLE</h1>
                        <h4 data-toc-skip class="author">Alexander
Towell</h4>
            
            <h4 data-toc-skip class="date">2025-12-17</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/queelius/compositional.mle/blob/HEAD/vignettes/theory-and-intuition.Rmd" class="external-link"><code>vignettes/theory-and-intuition.Rmd</code></a></small>
      <div class="d-none name"><code>theory-and-intuition.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="what-is-maximum-likelihood-estimation">What is Maximum Likelihood Estimation?<a class="anchor" aria-label="anchor" href="#what-is-maximum-likelihood-estimation"></a>
</h2>
<p>Maximum Likelihood Estimation (MLE) is a fundamental method for
estimating the parameters of a statistical model. The idea is simple yet
powerful: <strong>find the parameter values that make the observed data
most probable</strong>.</p>
<div class="section level3">
<h3 id="the-likelihood-function">The Likelihood Function<a class="anchor" aria-label="anchor" href="#the-likelihood-function"></a>
</h3>
<p>Suppose we observe data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, \ldots, x_n</annotation></semantics></math>
and we believe it comes from a probability distribution with
parameter(s)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
The <strong>likelihood function</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta)</annotation></semantics></math>
measures how probable the observed data is, given parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo>=</mo><msub><mi>x</mi><mi>n</mi></msub><mo>∣</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta) = P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n \mid \theta)</annotation></semantics></math></p>
<p>For independent observations:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta) = \prod_{i=1}^{n} f(x_i \mid \theta)</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo>∣</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot \mid \theta)</annotation></semantics></math>
is the probability density (or mass) function.</p>
</div>
<div class="section level3">
<h3 id="why-log-likelihood">Why Log-Likelihood?<a class="anchor" aria-label="anchor" href="#why-log-likelihood"></a>
</h3>
<p>Working with products is numerically unstable and mathematically
inconvenient. Taking the logarithm converts products to sums:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>log</mo><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo>log</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log f(x_i \mid \theta)</annotation></semantics></math></p>
<p>Since
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>log</mo><annotation encoding="application/x-tex">\log</annotation></semantics></math>
is monotonic, maximizing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\theta)</annotation></semantics></math>
is equivalent to maximizing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta)</annotation></semantics></math>.
The log-likelihood has several advantages:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Numerical stability</strong>: Products of small
probabilities can underflow to zero</li>
<li>
<strong>Computational efficiency</strong>: Sums are faster than
products</li>
<li>
<strong>Mathematical convenience</strong>: Derivatives of sums are
easier than derivatives of products</li>
<li>
<strong>Statistical properties</strong>: The curvature of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\theta)</annotation></semantics></math>
relates to estimation uncertainty</li>
</ol>
</div>
<div class="section level3">
<h3 id="a-concrete-example">A Concrete Example<a class="anchor" aria-label="anchor" href="#a-concrete-example"></a>
</h3>
<p>Let’s see this with normal data:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">50</span>, mean <span class="op">=</span> <span class="fl">3</span>, sd <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Log-likelihood function for normal distribution</span></span>
<span><span class="va">loglike</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">mu</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">sigma</span> <span class="op">&lt;=</span> <span class="fl">0</span><span class="op">)</span> <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="cn">Inf</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="va">x</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="va">sigma</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Visualize the log-likelihood surface</span></span>
<span><span class="va">mu_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">5</span>, length.out <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">sigma_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">3</span>, length.out <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">ll_surface</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/outer.html" class="external-link">outer</a></span><span class="op">(</span><span class="va">mu_grid</span>, <span class="va">sigma_grid</span>, <span class="kw">function</span><span class="op">(</span><span class="va">m</span>, <span class="va">s</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/mapply.html" class="external-link">mapply</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">mi</span>, <span class="va">si</span><span class="op">)</span> <span class="fu">loglike</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mi</span>, <span class="va">si</span><span class="op">)</span><span class="op">)</span>, <span class="va">m</span>, <span class="va">s</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Contour plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/contour.html" class="external-link">contour</a></span><span class="op">(</span><span class="va">mu_grid</span>, <span class="va">sigma_grid</span>, <span class="va">ll_surface</span>, nlevels <span class="op">=</span> <span class="fl">20</span>,</span>
<span>        xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">mu</span><span class="op">)</span>, ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span>,</span>
<span>        main <span class="op">=</span> <span class="st">"Log-Likelihood Surface"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/sd.html" class="external-link">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"red"</span>, cex <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, <span class="st">"MLE"</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<p><img src="theory-and-intuition_files/figure-html/normal-likelihood-1.png" class="r-plt" width="672"></p>
<p>The MLE is the point on this surface with the highest value (deepest
red in the contour plot).</p>
</div>
</div>
<div class="section level2">
<h2 id="the-score-function">The Score Function<a class="anchor" aria-label="anchor" href="#the-score-function"></a>
</h2>
<p>The <strong>score function</strong> is the gradient (vector of
partial derivatives) of the log-likelihood:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>∇</mi><mi>θ</mi></msub><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mo>ℓ</mo></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mn>1</mn></msub></mrow></mfrac><mo>,</mo><mi>…</mi><mo>,</mo><mfrac><mrow><mi>∂</mi><mo>ℓ</mo></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>p</mi></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">s(\theta) = \nabla_\theta \ell(\theta) = \left( \frac{\partial \ell}{\partial \theta_1}, \ldots, \frac{\partial \ell}{\partial \theta_p} \right)</annotation></semantics></math></p>
<p>At the MLE
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\theta}</annotation></semantics></math>,
the score is zero:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">s(\hat{\theta}) = 0</annotation></semantics></math>.</p>
<div class="section level3">
<h3 id="intuition">Intuition<a class="anchor" aria-label="anchor" href="#intuition"></a>
</h3>
<p>The score tells us the <strong>direction of steepest ascent</strong>
on the log-likelihood surface. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">s(\theta) \neq 0</annotation></semantics></math>,
we can increase the likelihood by moving in the direction of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">s(\theta)</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Score function for normal distribution</span></span>
<span><span class="va">score</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">mu</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span>,                        <span class="co"># d/d_mu</span></span>
<span>    <span class="op">-</span><span class="va">n</span> <span class="op">/</span> <span class="va">sigma</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">3</span>        <span class="co"># d/d_sigma</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># At a point away from the MLE, the score points toward the MLE</span></span>
<span><span class="va">theta_start</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.8</span><span class="op">)</span></span>
<span><span class="va">s</span> <span class="op">&lt;-</span> <span class="fu">score</span><span class="op">(</span><span class="va">theta_start</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Score at (1, 0.8):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">s</span>, <span class="fl">2</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Score at (1, 0.8): 152.07 593.01</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Direction: move"</span>, <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">s</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span>, <span class="st">"right"</span>, <span class="st">"left"</span><span class="op">)</span>, <span class="st">"in mu,"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">s</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span>, <span class="st">"up"</span>, <span class="st">"down"</span><span class="op">)</span>, <span class="st">"in sigma\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Direction: move right in mu, up in sigma</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="gradient-ascent">Gradient Ascent<a class="anchor" aria-label="anchor" href="#gradient-ascent"></a>
</h2>
<p><strong>Gradient ascent</strong> is the simplest optimization
algorithm. It iteratively moves in the direction of the gradient:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><mi>η</mi><mo>⋅</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta^{(t+1)} = \theta^{(t)} + \eta \cdot s(\theta^{(t)})</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\eta &gt; 0</annotation></semantics></math>
is the <strong>learning rate</strong> (step size).</p>
<div class="section level3">
<h3 id="why-it-works">Why It Works<a class="anchor" aria-label="anchor" href="#why-it-works"></a>
</h3>
<p>The score points in the direction of steepest increase. Taking small
steps in this direction guarantees improvement (for small enough
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>).</p>
</div>
<div class="section level3">
<h3 id="the-challenge-choosing-the-step-size">The Challenge: Choosing the Step Size<a class="anchor" aria-label="anchor" href="#the-challenge-choosing-the-step-size"></a>
</h3>
<ul>
<li>
<strong>Too large</strong>: We might overshoot and oscillate</li>
<li>
<strong>Too small</strong>: Convergence is painfully slow</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Demonstrate gradient ascent with different step sizes</span></span>
<span><span class="va">run_gradient_ascent</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">eta</span>, <span class="va">max_iter</span> <span class="op">=</span> <span class="fl">50</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.8</span><span class="op">)</span></span>
<span>  <span class="va">path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">max_iter</span> <span class="op">+</span> <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">path</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">theta</span></span>
<span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">max_iter</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">theta</span> <span class="op">&lt;-</span> <span class="va">theta</span> <span class="op">+</span> <span class="va">eta</span> <span class="op">*</span> <span class="fu">score</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="fl">0</span><span class="op">)</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0.01</span>  <span class="co"># Enforce constraint</span></span>
<span>    <span class="va">path</span><span class="op">[</span><span class="va">i</span> <span class="op">+</span> <span class="fl">1</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">theta</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">path</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Compare step sizes</span></span>
<span><span class="va">path_small</span> <span class="op">&lt;-</span> <span class="fu">run_gradient_ascent</span><span class="op">(</span><span class="fl">0.001</span><span class="op">)</span></span>
<span><span class="va">path_good</span> <span class="op">&lt;-</span> <span class="fu">run_gradient_ascent</span><span class="op">(</span><span class="fl">0.01</span><span class="op">)</span></span>
<span><span class="va">path_large</span> <span class="op">&lt;-</span> <span class="fu">run_gradient_ascent</span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot paths</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/contour.html" class="external-link">contour</a></span><span class="op">(</span><span class="va">mu_grid</span>, <span class="va">sigma_grid</span>, <span class="va">ll_surface</span>, nlevels <span class="op">=</span> <span class="fl">15</span>,</span>
<span>        xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">mu</span><span class="op">)</span>, ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span>,</span>
<span>        main <span class="op">=</span> <span class="st">"Gradient Ascent: Effect of Step Size"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="va">path_small</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">path_small</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="va">path_good</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">path_good</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"green"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="va">path_large</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">path_large</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="fl">2</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/sd.html" class="external-link">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span>, cex <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Small (0.001)"</span>, <span class="st">"Good (0.01)"</span>, <span class="st">"Large (0.05)"</span><span class="op">)</span>,</span>
<span>       col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"blue"</span>, <span class="st">"green"</span>, <span class="st">"red"</span><span class="op">)</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="theory-and-intuition_files/figure-html/gradient-demo-1.png" class="r-plt" width="672"></p>
</div>
<div class="section level3">
<h3 id="line-search-automatic-step-size-selection">Line Search: Automatic Step Size Selection<a class="anchor" aria-label="anchor" href="#line-search-automatic-step-size-selection"></a>
</h3>
<p><strong>Backtracking line search</strong> adaptively finds a good
step size:</p>
<ol style="list-style-type: decimal">
<li>Start with a large step size</li>
<li>If it doesn’t improve the objective enough, shrink it</li>
<li>Repeat until we find an acceptable step</li>
</ol>
<p>This is implemented in <code><a href="../reference/mle_config_linesearch.html">mle_config_linesearch()</a></code>:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numerical.mle/man/mle_gradient_ascent.html" class="external-link">mle_gradient_ascent</a></span><span class="op">(</span></span>
<span>  loglike <span class="op">=</span> <span class="va">loglike</span>,</span>
<span>  score <span class="op">=</span> <span class="va">score</span>,</span>
<span>  theta0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.8</span><span class="op">)</span>,</span>
<span>  config <span class="op">=</span> <span class="fu"><a href="../reference/mle_config_linesearch.html">mle_config_linesearch</a></span><span class="op">(</span>max_iter <span class="op">=</span> <span class="fl">50</span>, trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  constraint <span class="op">=</span> <span class="fu"><a href="../reference/mle_constraint.html">mle_constraint</a></span><span class="op">(</span></span>
<span>    support <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span>,</span>
<span>    project <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">1e-6</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Final estimate:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">result</span><span class="op">$</span><span class="va">theta.hat</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Final estimate: 2.9472 1.7103</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Iterations:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">iter</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Iterations: 8</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Converged:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">converged</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Converged: FALSE</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="the-fisher-information-matrix">The Fisher Information Matrix<a class="anchor" aria-label="anchor" href="#the-fisher-information-matrix"></a>
</h2>
<p>The <strong>Fisher information matrix</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\theta)</annotation></semantics></math>
measures how much information the data carries about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
It can be defined equivalently as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mo>ℓ</mo></mrow><mrow><mi>∂</mi><mi>θ</mi><mi>∂</mi><msup><mi>θ</mi><mi>T</mi></msup></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>s</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">I(\theta) = -E\left[ \frac{\partial^2 \ell}{\partial \theta \partial \theta^T} \right] = E\left[ s(\theta) s(\theta)^T \right]</annotation></semantics></math></p>
<p>The second form shows that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\theta)</annotation></semantics></math>
equals the covariance of the score (since
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">E[s(\theta)] = 0</annotation></semantics></math>
at the true parameter).</p>
<div class="section level3">
<h3 id="why-it-matters">Why It Matters<a class="anchor" aria-label="anchor" href="#why-it-matters"></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>Curvature</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\theta)</annotation></semantics></math>
describes the curvature of the log-likelihood surface</li>
<li>
<strong>Uncertainty</strong>: The asymptotic variance of the MLE is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>I</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\text{Var}(\hat{\theta}) \approx I(\theta)^{-1}</annotation></semantics></math>
(Cramér-Rao bound)</li>
<li>
<strong>Natural scaling</strong>: Different parameters may have
different scales;
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\theta)</annotation></semantics></math>
accounts for this</li>
</ol>
</div>
<div class="section level3">
<h3 id="observed-vs-expected-fisher-information">Observed vs Expected Fisher Information<a class="anchor" aria-label="anchor" href="#observed-vs-expected-fisher-information"></a>
</h3>
<ul>
<li>
<strong>Expected information</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>∇</mi><mn>2</mn></msup><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">I(\theta) = -E[\nabla^2 \ell(\theta)]</annotation></semantics></math>
</li>
<li>
<strong>Observed information</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><msup><mi>∇</mi><mn>2</mn></msup><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta) = -\nabla^2 \ell(\theta)</annotation></semantics></math>
(evaluated at the data)</li>
</ul>
<p>In practice, we often use the observed information, which doesn’t
require computing expectations.</p>
</div>
</div>
<div class="section level2">
<h2 id="newton-raphson">Newton-Raphson<a class="anchor" aria-label="anchor" href="#newton-raphson"></a>
</h2>
<p><strong>Newton-Raphson</strong> uses the Fisher information to take
smarter steps:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><mi>I</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta^{(t+1)} = \theta^{(t)} + I(\theta^{(t)})^{-1} s(\theta^{(t)})</annotation></semantics></math></p>
<div class="section level3">
<h3 id="intuition-1">Intuition<a class="anchor" aria-label="anchor" href="#intuition-1"></a>
</h3>
<p>Gradient ascent treats all directions equally, but some directions
might be “easier” to move in than others. Newton-Raphson pre-multiplies
by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>I</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">I^{-1}</annotation></semantics></math>,
which:</p>
<ul>
<li>Takes larger steps in flat directions (low curvature)</li>
<li>Takes smaller steps in curved directions (high curvature)</li>
<li>Accounts for correlations between parameters</li>
</ul>
</div>
<div class="section level3">
<h3 id="comparison">Comparison<a class="anchor" aria-label="anchor" href="#comparison"></a>
</h3>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fisher information for normal distribution</span></span>
<span><span class="va">fisher</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="va">n</span> <span class="op">/</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span>, <span class="fl">0</span>,</span>
<span>    <span class="fl">0</span>, <span class="fl">2</span> <span class="op">*</span> <span class="va">n</span> <span class="op">/</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Run gradient ascent</span></span>
<span><span class="va">result_ga</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numerical.mle/man/mle_gradient_ascent.html" class="external-link">mle_gradient_ascent</a></span><span class="op">(</span></span>
<span>  loglike <span class="op">=</span> <span class="va">loglike</span>,</span>
<span>  score <span class="op">=</span> <span class="va">score</span>,</span>
<span>  theta0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.8</span><span class="op">)</span>,</span>
<span>  config <span class="op">=</span> <span class="fu"><a href="../reference/mle_config_linesearch.html">mle_config_linesearch</a></span><span class="op">(</span>max_iter <span class="op">=</span> <span class="fl">100</span>, trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  constraint <span class="op">=</span> <span class="fu"><a href="../reference/mle_constraint.html">mle_constraint</a></span><span class="op">(</span></span>
<span>    support <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span>,</span>
<span>    project <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">1e-6</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Run Newton-Raphson</span></span>
<span><span class="va">result_nr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numerical.mle/man/mle_newton_raphson.html" class="external-link">mle_newton_raphson</a></span><span class="op">(</span></span>
<span>  loglike <span class="op">=</span> <span class="va">loglike</span>,</span>
<span>  score <span class="op">=</span> <span class="va">score</span>,</span>
<span>  fisher <span class="op">=</span> <span class="va">fisher</span>,</span>
<span>  theta0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.8</span><span class="op">)</span>,</span>
<span>  config <span class="op">=</span> <span class="fu"><a href="../reference/mle_config_linesearch.html">mle_config_linesearch</a></span><span class="op">(</span>max_iter <span class="op">=</span> <span class="fl">100</span>, trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  constraint <span class="op">=</span> <span class="fu"><a href="../reference/mle_constraint.html">mle_constraint</a></span><span class="op">(</span></span>
<span>    support <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span>,</span>
<span>    project <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">1e-6</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Gradient Ascent: "</span>, <span class="va">result_ga</span><span class="op">$</span><span class="va">iter</span>, <span class="st">"iterations\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Gradient Ascent:  8 iterations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Newton-Raphson:  "</span>, <span class="va">result_nr</span><span class="op">$</span><span class="va">iter</span>, <span class="st">"iterations\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Newton-Raphson:   11 iterations</span></span></code></pre></div>
<p>Newton-Raphson typically converges much faster, especially near the
optimum where its quadratic convergence kicks in.</p>
</div>
</div>
<div class="section level2">
<h2 id="when-to-use-which-method">When to Use Which Method<a class="anchor" aria-label="anchor" href="#when-to-use-which-method"></a>
</h2>
<div class="section level3">
<h3 id="use-gradient-ascent-when">Use Gradient Ascent When:<a class="anchor" aria-label="anchor" href="#use-gradient-ascent-when"></a>
</h3>
<ul>
<li>You don’t have (or can’t easily compute) the Fisher information</li>
<li>The problem is high-dimensional (computing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>I</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">I^{-1}</annotation></semantics></math>
is expensive)</li>
<li>You’re using stochastic gradients (mini-batches)</li>
<li>Getting a rough answer quickly is acceptable</li>
</ul>
</div>
<div class="section level3">
<h3 id="use-newton-raphson-when">Use Newton-Raphson When:<a class="anchor" aria-label="anchor" href="#use-newton-raphson-when"></a>
</h3>
<ul>
<li>You need high precision</li>
<li>The Fisher information is available and cheap to compute</li>
<li>The problem is low-to-moderate dimensional</li>
<li>Fast convergence is important</li>
</ul>
</div>
<div class="section level3">
<h3 id="use-grid-search-when">Use Grid Search When:<a class="anchor" aria-label="anchor" href="#use-grid-search-when"></a>
</h3>
<ul>
<li>The parameter space is small (1-3 dimensions)</li>
<li>You need a good starting point</li>
<li>The likelihood surface might have multiple modes</li>
</ul>
</div>
<div class="section level3">
<h3 id="use-random-restart-when">Use Random Restart When:<a class="anchor" aria-label="anchor" href="#use-random-restart-when"></a>
</h3>
<ul>
<li>The likelihood surface has multiple local maxima</li>
<li>You’re uncertain about good starting values</li>
<li>Robustness is more important than speed</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="constrained-optimization">Constrained Optimization<a class="anchor" aria-label="anchor" href="#constrained-optimization"></a>
</h2>
<p>Real problems often have <strong>constraints</strong> on
parameters:</p>
<ul>
<li>Variance must be positive:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma &gt; 0</annotation></semantics></math>
</li>
<li>Probabilities must be in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math>
</li>
<li>Correlation must satisfy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>ρ</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|\rho| &lt; 1</annotation></semantics></math>
</li>
</ul>
<div class="section level3">
<h3 id="projection-method">Projection Method<a class="anchor" aria-label="anchor" href="#projection-method"></a>
</h3>
<p>The <code>numerical.mle</code> package uses
<strong>projection</strong>: if a step takes us outside the feasible
region, we project back to the nearest feasible point.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Without constraint: optimization might fail</span></span>
<span><span class="va">constraint</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_constraint.html">mle_constraint</a></span><span class="op">(</span></span>
<span>  support <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span>,</span>
<span>  project <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">1e-6</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The constraint keeps sigma positive throughout optimization</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numerical.mle/man/mle_gradient_ascent.html" class="external-link">mle_gradient_ascent</a></span><span class="op">(</span></span>
<span>  loglike <span class="op">=</span> <span class="va">loglike</span>,</span>
<span>  score <span class="op">=</span> <span class="va">score</span>,</span>
<span>  theta0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.1</span><span class="op">)</span>,  <span class="co"># Start near the boundary</span></span>
<span>  config <span class="op">=</span> <span class="fu"><a href="../reference/mle_config_linesearch.html">mle_config_linesearch</a></span><span class="op">(</span>max_iter <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>,</span>
<span>  constraint <span class="op">=</span> <span class="va">constraint</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Final sigma:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">theta.hat</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="st">"&gt; 0 (constraint satisfied)\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Final sigma: 1.710644 &gt; 0 (constraint satisfied)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="regularization-and-penalized-likelihood">Regularization and Penalized Likelihood<a class="anchor" aria-label="anchor" href="#regularization-and-penalized-likelihood"></a>
</h2>
<p>Sometimes we want to <strong>penalize</strong> certain parameter
values to:</p>
<ul>
<li>Prevent overfitting</li>
<li>Encourage sparsity</li>
<li>Incorporate prior beliefs</li>
</ul>
<div class="section level3">
<h3 id="penalized-log-likelihood">Penalized Log-Likelihood<a class="anchor" aria-label="anchor" href="#penalized-log-likelihood"></a>
</h3>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>ℓ</mo><mi>λ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>λ</mi><mo>⋅</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell_\lambda(\theta) = \ell(\theta) - \lambda \cdot P(\theta)</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\theta)</annotation></semantics></math>
is a penalty function and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda &gt; 0</annotation></semantics></math>
controls regularization strength.</p>
</div>
<div class="section level3">
<h3 id="common-penalties">Common Penalties<a class="anchor" aria-label="anchor" href="#common-penalties"></a>
</h3>
<p><strong>L1 (LASSO)</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mo>∑</mo><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>θ</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">P(\theta) = \sum_j |\theta_j|</annotation></semantics></math></p>
<ul>
<li>Encourages sparsity (some
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\theta_j = 0</annotation></semantics></math>)</li>
<li>Useful for variable selection</li>
</ul>
<p><strong>L2 (Ridge)</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mo>∑</mo><mi>j</mi></msub><msubsup><mi>θ</mi><mi>j</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">P(\theta) = \sum_j \theta_j^2</annotation></semantics></math></p>
<ul>
<li>Shrinks parameters toward zero</li>
<li>Prevents extreme values</li>
<li>Equivalent to Gaussian prior</li>
</ul>
<p><strong>Elastic Net</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>α</mi><msub><mo>∑</mo><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>θ</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo>∑</mo><mi>j</mi></msub><msubsup><mi>θ</mi><mi>j</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">P(\theta) = \alpha \sum_j |\theta_j| + (1-\alpha) \sum_j \theta_j^2</annotation></semantics></math></p>
<ul>
<li>Combines L1 and L2 benefits</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
controls the mix</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Original log-likelihood (maximum at theta = 0)</span></span>
<span><span class="va">loglike</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">theta</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add L2 penalty</span></span>
<span><span class="va">loglike_l2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/with_penalty.html">with_penalty</a></span><span class="op">(</span><span class="va">loglike</span>, <span class="fu"><a href="../reference/penalty_l2.html">penalty_l2</a></span><span class="op">(</span><span class="op">)</span>, lambda <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"At theta = (3, 2):\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; At theta = (3, 2):</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"  Original:"</span>, <span class="fu">loglike</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Original: 0</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"  With L2 penalty:"</span>, <span class="fu">loglike_l2</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt;   With L2 penalty: -13</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"  The penalty shrinks the solution toward zero\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt;   The penalty shrinks the solution toward zero</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="stochastic-gradient-methods">Stochastic Gradient Methods<a class="anchor" aria-label="anchor" href="#stochastic-gradient-methods"></a>
</h2>
<p>For large datasets, computing the full gradient is expensive.
<strong>Stochastic gradient ascent</strong> uses random subsamples:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><mi>η</mi><mo>⋅</mo><mover><mi>s</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta^{(t+1)} = \theta^{(t)} + \eta \cdot \hat{s}(\theta^{(t)})</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>s</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{s}</annotation></semantics></math>
is computed on a mini-batch of data.</p>
<div class="section level3">
<h3 id="properties">Properties<a class="anchor" aria-label="anchor" href="#properties"></a>
</h3>
<ul>
<li>
<strong>Noisy but unbiased</strong>: On average, we move in the
right direction</li>
<li>
<strong>Faster iterations</strong>: Each step is cheap</li>
<li>
<strong>Implicit regularization</strong>: Noise can help escape
local optima</li>
<li>
<strong>Requires care</strong>: Learning rate schedules, momentum,
etc.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Concept demonstration (not run)</span></span>
<span><span class="va">loglike_full</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>, <span class="va">obs</span> <span class="op">=</span> <span class="va">large_data</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="va">obs</span>, <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Use only 100 observations per iteration</span></span>
<span><span class="va">loglike_mini</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/with_subsampling.html">with_subsampling</a></span><span class="op">(</span></span>
<span>  <span class="va">loglike_full</span>,</span>
<span>  data <span class="op">=</span> <span class="va">large_data</span>,</span>
<span>  subsample_size <span class="op">=</span> <span class="fl">100</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="convergence-criteria">Convergence Criteria<a class="anchor" aria-label="anchor" href="#convergence-criteria"></a>
</h2>
<p>The package checks for convergence using:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Absolute tolerance</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mo>&lt;</mo><msub><mi>ϵ</mi><mrow><mi>a</mi><mi>b</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">|\ell(\theta^{(t+1)}) - \ell(\theta^{(t)})| &lt; \epsilon_{abs}</annotation></semantics></math>
</li>
<li>
<strong>Relative tolerance</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow></mfrac><mo>&lt;</mo><msub><mi>ϵ</mi><mrow><mi>r</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\frac{|\ell(\theta^{(t+1)}) - \ell(\theta^{(t)})|}{|\ell(\theta^{(t)})|} &lt; \epsilon_{rel}</annotation></semantics></math>
</li>
<li>
<strong>Parameter change</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>−</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="false" form="postfix">∥</mo><mo>&lt;</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\|\theta^{(t+1)} - \theta^{(t)}\| &lt; \epsilon</annotation></semantics></math>
</li>
<li>
<strong>Maximum iterations</strong>: Safety stop</li>
</ol>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">config</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_config.html">mle_config</a></span><span class="op">(</span></span>
<span>  max_iter <span class="op">=</span> <span class="fl">500</span>,     <span class="co"># Don't run forever</span></span>
<span>  abs_tol <span class="op">=</span> <span class="fl">1e-8</span>,     <span class="co"># Absolute change threshold</span></span>
<span>  rel_tol <span class="op">=</span> <span class="fl">1e-6</span>,     <span class="co"># Relative change threshold</span></span>
<span>  debug <span class="op">=</span> <span class="cn">FALSE</span>       <span class="co"># Set TRUE to see iteration progress</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<table class="table">
<colgroup>
<col width="18%">
<col width="15%">
<col width="43%">
<col width="22%">
</colgroup>
<thead><tr class="header">
<th>Method</th>
<th>Order</th>
<th>Information Needed</th>
<th>Best For</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Gradient Ascent</td>
<td>1st</td>
<td>Score only</td>
<td>Large problems, rough estimates</td>
</tr>
<tr class="even">
<td>Newton-Raphson</td>
<td>2nd</td>
<td>Score + Fisher</td>
<td>High precision, small problems</td>
</tr>
<tr class="odd">
<td>Grid Search</td>
<td>0th</td>
<td>Likelihood only</td>
<td>Finding starting points</td>
</tr>
<tr class="even">
<td>Random Restart</td>
<td>-</td>
<td>Varies</td>
<td>Multi-modal problems</td>
</tr>
</tbody>
</table>
<p>The <code>numerical.mle</code> package provides a unified interface
to all these methods, with composable configuration objects and function
transformers to handle real-world complexities like constraints,
regularization, and large datasets.</p>
</div>
<div class="section level2">
<h2 id="further-reading">Further Reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<ol style="list-style-type: decimal">
<li>Casella, G. and Berger, R.L. (2002). <em>Statistical Inference</em>.
Duxbury.</li>
<li>Nocedal, J. and Wright, S.J. (2006). <em>Numerical
Optimization</em>. Springer.</li>
<li>Murphy, K.P. (2012). <em>Machine Learning: A Probabilistic
Perspective</em>. MIT Press.</li>
</ol>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Alexander Towell.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
