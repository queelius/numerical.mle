# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Package Overview

`numerical.mle` is an R package providing numerical maximum likelihood estimation (MLE) solvers. The package implements various optimization algorithms to find MLEs from log-likelihood functions. This is early alpha software that was extracted from the `algebraic.mle` package.

Key dependency: This package builds on `algebraic.mle` for the core `mle` object representation.

## Development Commands

### Building and Documentation
- **Build package**: `R CMD build .` or in R: `devtools::build()`
- **Install locally**: `devtools::install()` or `devtools::install_github("queelius/numerical.mle")`
- **Load for development**: `devtools::load_all()`
- **Generate documentation**: `devtools::document()` (runs roxygen2)
- **Build README**: Knit `README.Rmd` to generate `README.md`
- **Build pkgdown site**: `pkgdown::build_site()`

### Testing
- **Run all tests**: `devtools::test()` or `testthat::test_dir("tests/testthat")`
- **Run specific test file**: `testthat::test_file("tests/testthat/test-*.R")`
- **Check package**: `devtools::check()` (comprehensive R CMD check)
- **Test coverage**: `covr::package_coverage()` to analyze test coverage

### Code Quality
- **Lint code**: Use `lintr::lint_package()` for style checking
- **Check NAMESPACE**: Auto-generated by roxygen2 - never edit manually

## Architecture

### Core Object Model
All numerical MLE solvers return `mle_numerical` objects, which extend the base `mle` class from `algebraic.mle`. The `mle_numerical` class adds:
- `iter`: number of iterations taken
- `converged`: logical indicating convergence
- `options`: configuration used during optimization

### Solver Hierarchy
The package implements a layered architecture:

1. **Base Layer** (`R/numerical.mle.R`, `R/generic_functions.R`):
   - `mle_numerical()`: Constructor for numerical MLE objects
   - Generic methods: `is_converged()`, `num_iterations()`, `is_mle_numerical()`

2. **Utility Layer** (`R/utils.R`):
   - `backtracking_line_search()`: Adaptive step size for gradient methods
   - `clip_step()`: Step size limiting
   - Various optimization utilities (grad_descent, local_minimize_ls, etc.)

3. **Core Solver** (`R/mle_local_search.R`):
   - `mle_local_search()`: Generalized local search framework used by most solvers
   - Configurable via `options` list with parameters for convergence, step size, line search, debugging, etc.
   - Supports projection functions for constrained optimization
   - Optional path tracing for visualization

4. **Specialized Solvers**:
   - `mle_gradient_ascent()`: Gradient-based optimization (uses score function)
   - `mle_newton_raphson()`: Second-order Newton-Raphson (uses score and Fisher information)
   - `mle_grid_search()`: Exhaustive grid search over parameter space
   - `mle_random_search()`: Stochastic search methods
   - `mle_random_restart()`: Multiple random initializations
   - `mle_sim_anneal()`: Simulated annealing for global optimization
   - `mle_optim()`: Wrapper for base R `optim()` that returns `mle` objects

### Key Design Patterns

**Function Adapters**: The package provides adapters for log-likelihood functions:
- `stochastic_loglike()`: Subsample observations for large datasets (enables stochastic gradient ascent)
- `penalize_loglike()`: Add penalty terms for constrained optimization

**Options Pattern**: Most solvers accept an `options` list for configuration. Common options include:
- `sup`: Support constraint function (domain checking)
- `proj`: Projection function to enforce constraints
- `loglike`: Log-likelihood function (required for line search)
- `eta`: Learning rate/step size
- `max_iter`: Maximum iterations
- `abs_tol`/`rel_tol`: Convergence tolerances
- `debug`: Enable debugging output
- `trace`: Store optimization path
- `line_search`: Enable backtracking line search

**Direction Functions**: Local search methods accept a `dir` function that computes a promising search direction:
- Gradient ascent uses `dir = score` (gradient of log-likelihood)
- Newton-Raphson uses `dir = function(x) covar(x) %*% score(x)` (FIM-weighted gradient)

### File Organization
- `R/mle_*.R`: Individual solver implementations
- `R/generic_functions.R`: Generic methods for `mle_numerical` objects
- `R/utils.R`: Low-level optimization utilities
- `man/*.Rd`: Auto-generated documentation (via roxygen2)
- `vignettes/`: Extended examples and tutorials
- `fixing/`: Development/experimental code (not part of package)

## Important Notes

- This is **alpha software** - many functions may not be fully tested or working
- Always provide initial guesses (`theta0`) that are in the support of the log-likelihood
- For constrained optimization, use the `sup` (support checker) and `proj` (projection) options
- Newton-Raphson requires both score and Fisher information matrix functions
- Grid search can be combined with local search via the `mle_solver` option
- The package expects `algebraic.mle` to be installed for the base `mle` class
